---
title: "Assignment3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("glmnet")
#install.packages("psych")
#install.packages("Metrics")
```



```{r}
require(data.table)
set.seed(123)
library(glmnet)  # for ridge regression
library(dplyr)   # for data cleaning
library(psych)
library(Metrics)
```

<h1>Question 4<h1>
<h1>a)</h>
When the model fits the training data but does not have a good predicting performance and generalization power, we use Regularisation. It is mostly used to avoid overfitting.


```{r}
data1=fread("train.csv")
data_test=fread("test.csv")

# Center y, X will be standardized in the modelling function
y <- data1 %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- data1 %>% select(-SalePrice) %>% as.matrix()


# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 0 implements ridge regression
ridge_cv <- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
#X is the matrix of input observations
#y is the vector of responses
#lambda is the vector of penalty terms

# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min


y2 <- data_test %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X2 <- data_test %>% select(-SalePrice) %>% as.matrix()

#fitting the model for test data
test_cv <- glmnet(X2, y2, alpha = 0, lambda = lambda_cv, standardize = TRUE)

y_hat <- predict(test_cv, X2)
ssr <- t(y2 - y_hat) %*% (y2 - y_hat)
rsq_ridge <- cor(y2, y_hat)^2

rmse_=rmse(y2,y_hat)

print(rsq_ridge)
print(rmse_)
```



```{r}
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 6, length.out = 100)

# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)

# Best cross-validated lambda
lambda_lasso <- lasso_cv$lambda.min

#fitting the model for test data
test_lasso_cv <- glmnet(X2, y2, alpha = 0, lambda = lambda_lasso, standardize = TRUE)

#predicted values of y for test data
y_hat_lasso <- predict(test_lasso_cv, X2)
ssr_lasso <- t(y2 - y_hat_lasso) %*% (y2 - y_hat_lasso)
rsq_lasso <- cor(y2, y_hat_lasso)^2
rmse_2=rmse(y2,y_hat_lasso)
print(rsq_lasso)
print(rmse_2)
```

<h2>d)</h2>
Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for feature selection because coefficient values can be zero.

