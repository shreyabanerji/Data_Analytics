ssr_lasso <- t(y2 - y_hat_lasso) %*% (y2 - y_hat_lasso)
rsq_lasso <- cor(y2, y_hat_lasso)^2
rmse_2=rmse(y2,y_hat_lasso)
print(rsq_lasso)
print(rmse_2)
#d)
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#q1
#install.packages("corrgram")
#install.packages("Metrics")
#install.packages('caret', repos='http://cran.us.r-project.org')
#install.packages("PCAmixdata")
#q4
#install.packages("glmnet")
#install.packages("psych")
#install.packages("Metrics")
library("corrgram")
library("Metrics")
library("dplyr")
library("corrplot")
library(PCAmixdata)
require(data.table)
set.seed(123)
library(glmnet)
library(dplyr)
library(psych)
library(Metrics)
knitr::opts_chunk$set(echo = TRUE)
#b) Correlogram visualization
#reference : https://www.statmethods.net/advgraphs/correlograms.html ; https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
test_df  = read.csv("AmesHousingPrices/test.csv")
train_df  = read.csv("AmesHousingPrices/train.csv")
#taking only the quantitative features
test_quantitative = select_if(test_df, is.numeric) #this takes only the quantitative values from the df
train_quantitative = select_if(train_df, is.numeric)
M <- cor(test_quantitative)
corrplot(M, method = "circle")
N <- cor(train_quantitative)
corrplot(N, method = "circle")
#correlation cutoff
#Reference : https://stackoverflow.com/questions/18275639/remove-highly-correlated-variables
#Ref: https://stackoverflow.com/questions/5863097/selecting-only-numeric-columns-from-a-data-frame
#taking only the quantitative features
split1 <- splitmix(test_df)
split2 <- splitmix(train_df)
test_qualitative =split1$X.quali #this takes only the quantitative values from the df
train_qualitative = split2$X.quali
#print(test_qualitative)
#print(train_qualitative)
train_qualitative$SalePrice = train_df$SalePrice
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Street), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Street",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$RoofStyle), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Roof Style",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Foundation), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Foundation",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Heating), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Heating",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$CentralAir), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Airconditioning",names.arg=avg$Street)
#LINEAR MODEL 1
linear_model1=lm(SalePrice ~ LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model1))
res = resid(linear_model1)
plot(res)
hist(train_df$SalePrice,xlab="SalePrice",main="Histogram of Sale Price")
hist(log1p(train_df$SalePrice),xlab="log(SalePrice)",main="Histogram of log(Sale Price)")
#LINEAR MODEL 2
#The second model is built after applying log transformation on the dependent variable
linear_model2=lm(log1p(SalePrice) ~LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model2))
res2 = resid(linear_model2)
plot(res2)
#calculating mae for test.csv
predictions1 = predict.lm(linear_model1, test_df)
predictions2 = predict.lm(linear_model2, test_df)
mae1=mae(test_df$SalePrice,predictions1)
mae2=mae(test_df$SalePrice,exp(predictions2))
print(mae1)    #before the transformation
print(mae2)   #after the transformation
data1=fread("ShenzhenHousingPrices/train.csv")
data_test=fread("ShenzhenHousingPrices/test.csv")
# Center y, X will be standardized in the modelling function
y <- data1 %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X1 <- data1 %>% select(-SalePrice)
X <-model.matrix(~.,data=X1)  #assigns dummy variables to the factor and leaves the numeric variables alone
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 0 implements ridge regression
ridge_cv <- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
#X is the matrix of input observations
#y is the vector of responses
#lambda is the vector of penalty terms
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
y2 <- data_test %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X_ <- data_test %>% select(-SalePrice)
X2 <-model.matrix(~.,data=X_)
#fitting the model for test data
test_cv <- glmnet(X2, y2, alpha = 0, lambda = lambda_cv, standardize = TRUE)
y_hat <- predict(test_cv, X2)
ssr <- t(y2 - y_hat) %*% (y2 - y_hat)
rsq_ridge <- cor(y2, y_hat)^2
rmse_=rmse(y2,y_hat)
print(rsq_ridge)
print(rmse_)
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 6, length.out = 100)
# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Best cross-validated lambda
lambda_lasso <- lasso_cv$lambda.min
#fitting the model for test data
test_lasso_cv <- glmnet(X2, y2, alpha = 1, lambda = lambda_lasso, standardize = TRUE)
#predicted values of y for test data
y_hat_lasso <- predict(test_lasso_cv, X2)
ssr_lasso <- t(y2 - y_hat_lasso) %*% (y2 - y_hat_lasso)
rsq_lasso <- cor(y2, y_hat_lasso)^2
rmse_2=rmse(y2,y_hat_lasso)
print(rsq_lasso)
print(rmse_2)
library("corrgram")
library("Metrics")
library("dplyr")
library("corrplot")
library(PCAmixdata)
require(data.table)
set.seed(123)
library(glmnet)
library(dplyr)
library(psych)
library(Metrics)
#Correlogram visualization
#reference : https://www.statmethods.net/advgraphs/correlograms.html ; https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
test_df  = read.csv("AmesHousingPrices/test.csv")
train_df  = read.csv("AmesHousingPrices/train.csv")
#taking only the quantitative features
test_quantitative = select_if(test_df, is.numeric) #this takes only the quantitative values from the df
train_quantitative = select_if(train_df, is.numeric)
M <- cor(test_quantitative)
corrplot(M, method = "circle")
N <- cor(train_quantitative)
corrplot(N, method = "circle")
#taking only the quantitative features
split1 <- splitmix(test_df)
split2 <- splitmix(train_df)
test_qualitative =split1$X.quali #this takes only the quantitative values from the df
train_qualitative = split2$X.quali
train_qualitative$SalePrice = train_df$SalePrice
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Street), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Street",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$RoofStyle), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Roof Style",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Foundation), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Foundation",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Heating), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Heating",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$CentralAir), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Airconditioning",names.arg=avg$Street)
linear_model1=lm(SalePrice ~ LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model1))
res = resid(linear_model1)
plot(res)
#c)
hist(train_df$SalePrice,xlab="SalePrice",main="Histogram of Sale Price")
#Since the histogram is right skewed, log transformation can be applied to make it normal.
hist(log1p(train_df$SalePrice),xlab="log(SalePrice)",main="Histogram of log(Sale Price)")
#LINEAR MODEL 2
#The second model is built after applying log transformation on the dependent variable
linear_model2=lm(log1p(SalePrice) ~LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model2))
res2 = resid(linear_model2)
plot(res2)
#b)
#calculating mae for test.csv
predictions1 = predict.lm(linear_model1, test_df)
predictions2 = predict.lm(linear_model2, test_df)
mae1=mae(test_df$SalePrice,predictions1)
mae2=mae(test_df$SalePrice,exp(predictions2))
print(mae1)    #before the transformation
print(mae2)   #after the transformation
#b)
data1=fread("ShenzhenHousingPrices/train.csv")
data_test=fread("ShenzhenHousingPrices/test.csv")
# Center y, X will be standardized in the modelling function
y <- data1 %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X1 <- data1 %>% select(-SalePrice)
X <-model.matrix(~.,data=X1)  #assigns dummy variables to the factor and leaves the numeric variables alone
#RidgeRegression
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 0 implements ridge regression
ridge_cv <- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
y2 <- data_test %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X_ <- data_test %>% select(-SalePrice)
X2 <-model.matrix(~.,data=X_)
#fitting the model for test data
test_cv <- glmnet(X2, y2, alpha = 0, lambda = lambda_cv, standardize = TRUE)
y_hat <- predict(test_cv, X2)
ssr <- t(y2 - y_hat) %*% (y2 - y_hat)
rsq_ridge <- cor(y2, y_hat)^2
rmse_=rmse(y2,y_hat)
print(rsq_ridge)
print(rmse_)
#c)
#Lasso Regression
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 6, length.out = 100)
# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Best cross-validated lambda
lambda_lasso <- lasso_cv$lambda.min
#fitting the model for test data
test_lasso_cv <- glmnet(X2, y2, alpha = 1, lambda = lambda_lasso, standardize = TRUE)
#predicted values of y for test data
y_hat_lasso <- predict(test_lasso_cv, X2)
ssr_lasso <- t(y2 - y_hat_lasso) %*% (y2 - y_hat_lasso)
rsq_lasso <- cor(y2, y_hat_lasso)^2
rmse_2=rmse(y2,y_hat_lasso)
print(rsq_lasso)
print(rmse_2)
#d)
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#d)
#Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.
#Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
#“Ridge regression” uses all predictors in the final model whereas “Lasso regression” can be used for
#feature selection because coefficient values can be zero.
#q1
#install.packages("corrgram")
#install.packages("Metrics")
#install.packages('caret', repos='http://cran.us.r-project.org')
#install.packages("PCAmixdata")
#q4
#install.packages("glmnet")
#install.packages("psych")
#install.packages("Metrics")
path="/Users/shreyabanerjee/DA_Assignments/Assignment3"
library("corrgram")
library("Metrics")
library("dplyr")
library("corrplot")
library(PCAmixdata)
require(data.table)
set.seed(123)
library(glmnet)
library(dplyr)
library(psych)
library(Metrics)
knitr::opts_chunk$set(echo = TRUE)
#b) Correlogram visualization
#reference : https://www.statmethods.net/advgraphs/correlograms.html ; https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
test_df  = read.csv("AmesHousingPrices/test.csv")
train_df  = read.csv("AmesHousingPrices/train.csv")
#taking only the quantitative features
test_quantitative = select_if(test_df, is.numeric) #this takes only the quantitative values from the df
train_quantitative = select_if(train_df, is.numeric)
M <- cor(test_quantitative)
corrplot(M, method = "circle")
N <- cor(train_quantitative)
corrplot(N, method = "circle")
#correlation cutoff
#Reference : https://stackoverflow.com/questions/18275639/remove-highly-correlated-variables
#Ref: https://stackoverflow.com/questions/5863097/selecting-only-numeric-columns-from-a-data-frame
#taking only the quantitative features
split1 <- splitmix(test_df)
split2 <- splitmix(train_df)
test_qualitative =split1$X.quali #this takes only the quantitative values from the df
train_qualitative = split2$X.quali
#print(test_qualitative)
#print(train_qualitative)
train_qualitative$SalePrice = train_df$SalePrice
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Street), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Street",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$RoofStyle), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Roof Style",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Foundation), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Foundation",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Heating), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Heating",names.arg=avg$Street)
avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$CentralAir), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Airconditioning",names.arg=avg$Street)
#LINEAR MODEL 1
linear_model1=lm(SalePrice ~ LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model1))
res = resid(linear_model1)
plot(res)
hist(train_df$SalePrice,xlab="SalePrice",main="Histogram of Sale Price")
hist(log1p(train_df$SalePrice),xlab="log(SalePrice)",main="Histogram of log(Sale Price)")
#LINEAR MODEL 2
#The second model is built after applying log transformation on the dependent variable
linear_model2=lm(log1p(SalePrice) ~LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model2))
res2 = resid(linear_model2)
plot(res2)
#calculating mae for test.csv
predictions1 = predict.lm(linear_model1, test_df)
predictions2 = predict.lm(linear_model2, test_df)
mae1=mae(test_df$SalePrice,predictions1)
mae2=mae(test_df$SalePrice,exp(predictions2))
print(mae1)    #before the transformation
print(mae2)   #after the transformation
data1=fread("ShenzhenHousingPrices/train.csv")
data_test=fread("ShenzhenHousingPrices/test.csv")
# Center y, X will be standardized in the modelling function
y <- data1 %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X1 <- data1 %>% select(-SalePrice)
X <-model.matrix(~.,data=X1)  #assigns dummy variables to the factor and leaves the numeric variables alone
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 0 implements ridge regression
ridge_cv <- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
#X is the matrix of input observations
#y is the vector of responses
#lambda is the vector of penalty terms
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
y2 <- data_test %>% select(SalePrice) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X_ <- data_test %>% select(-SalePrice)
X2 <-model.matrix(~.,data=X_)
#fitting the model for test data
test_cv <- glmnet(X2, y2, alpha = 0, lambda = lambda_cv, standardize = TRUE)
y_hat <- predict(test_cv, X2)
ssr <- t(y2 - y_hat) %*% (y2 - y_hat)
rsq_ridge <- cor(y2, y_hat)^2
rmse_=rmse(y2,y_hat)
print(rsq_ridge)
print(rmse_)
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 6, length.out = 100)
# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Best cross-validated lambda
lambda_lasso <- lasso_cv$lambda.min
#fitting the model for test data
test_lasso_cv <- glmnet(X2, y2, alpha = 1, lambda = lambda_lasso, standardize = TRUE)
#predicted values of y for test data
y_hat_lasso <- predict(test_lasso_cv, X2)
ssr_lasso <- t(y2 - y_hat_lasso) %*% (y2 - y_hat_lasso)
rsq_lasso <- cor(y2, y_hat_lasso)^2
rmse_2=rmse(y2,y_hat_lasso)
print(rsq_lasso)
print(rmse_2)
