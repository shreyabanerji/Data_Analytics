---
title: "Assignment 3"
author: "Ishwar Choudhary & Shreya Banerjee"
date: "25/09/2019"
output: html_document
---
```{r}
#install.packages("corrgram")
#install.packages("Metrics")
library("corrgram")
library("Metrics")
```


```{r}
library("dplyr")
library("corrplot")

#install.packages('caret', repos='http://cran.us.r-project.org')


```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
#Question 1
<h1> Question 1 </h1>
a) Why is it necessary to do away with multicollinearity?
Ans : When independent variables are correlated, it indicates that changes in one variable are associated with shifts in another variable. 
The stronger the correlation, the more difficult it is to change one variable without changing another. Hence it becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend change and affect.

b) Select only the quantitative variables and plot a correlogram to visualise the degree of
correlation between pairs of variables. If you were to drop variables based on this plot, 
which one(s) would you drop and why? Go ahead and drop them from the train and test set. (Consider a threshold of 0.8)
```{r}
#b) Correlogram visualization
#reference : https://www.statmethods.net/advgraphs/correlograms.html ; https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
test_df  = read.csv("test.csv")
train_df  = read.csv("train.csv")

#taking only the quantitative features
test_quantitative = select_if(test_df, is.numeric) #this takes only the quantitative values from the df
train_quantitative = select_if(train_df, is.numeric)

M <- cor(test_quantitative)
corrplot(M, method = "circle")

N <- cor(train_quantitative)
corrplot(N, method = "circle")

#correlation cutoff
#Reference : https://stackoverflow.com/questions/18275639/remove-highly-correlated-variables
#dropping of variables not working
#caret not working

```
Looking at the data, we can say that we can safely drop SqFootage, TotRmsAbvGrd as they have a very high correlation with all the other variables.
Note: Should we drop the SqFootage ?? COz it is pretty important
```{r}

```

c) 
Pearson’s correlation coefficient cannot be used on categorical variables. Provide a suitable visualisation for the relationship between a categorical variable and the target variable. Plot this graph for the variable “OverallQuality” and indicate its relationship with the target “SalePrice”. Why is this variable categorical?

```{r}
#Ref: https://stackoverflow.com/questions/5863097/selecting-only-numeric-columns-from-a-data-frame
#install.packages("PCAmixdata")
library(PCAmixdata)
```


```{r}
#taking only the quantitative features
split1 <- splitmix(test_df)
split2 <- splitmix(train_df)
test_qualitative =split1$X.quali #this takes only the quantitative values from the df
train_qualitative = split2$X.quali
#print(test_qualitative)
print(train_qualitative)

train_qualitative$SalePrice = train_df$SalePrice

avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Street), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Street",names.arg=avg$Street)

avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$RoofStyle), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Roof Style",names.arg=avg$Street)

avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Foundation), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Foundation",names.arg=avg$Street)

avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$Heating), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Heating",names.arg=avg$Street)

avg = aggregate(train_qualitative$SalePrice, by=list(Street=train_qualitative$CentralAir), FUN=mean)
barplot(avg$x, main="Sale Prices",ylab="Prices",xlab="Type of Airconditioning",names.arg=avg$Street)


#need to visualise properly
```
#Question 2
<h1> Question 2 </h1>
a) Fit a linear regression model to your data, using all of the variables you decided to keep from the previous question

```{r}
#LINEAR MODEL 1

linear_model1=lm(SalePrice ~ LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model1))
res = resid(linear_model1)
plot(res)
```
<br><h2> b)</h2>An ideal residual plot should not have any trend or any outliers. Here, most of the point are concentrated around the zero but one point is far away(outlier).<br>
<h2>c)</h2>
```{r}
hist(train_df$SalePrice,xlab="SalePrice",main="Histogram of Sale Price")
```
Since the histogram is right skewed, log transformation can be applied to make it normal.

```{r}
hist(log1p(train_df$SalePrice),xlab="log(SalePrice)",main="Histogram of log(Sale Price)")
```

The histogram attains normal distribution after applying log transformation.

```{r}
#LINEAR MODEL 2
#The second model is built after applying log transformation on the dependent variable
linear_model2=lm(log1p(SalePrice) ~LotArea+OverallQual+factor(RoofStyle)+factor(Foundation)+factor(CentralAir)+GrLivArea+FullBath+HalfBath+BedroomAbvGr+factor(KitchenQual)+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+PoolArea+SqFootage, data = train_df)
print(summary(linear_model2))
res2 = resid(linear_model2)
plot(res2)
```
The residual plot of linear model2(i.e,after applying log transformation) does not show any trend. It is more scattered than the residual plot linear model1 i.e, the model before applying transformation.This can be inferred by observing the range of y-axis in the residual plots.

<h2>d)</h2>
For model 1, we can clearly detect an outlier. Moreover the residual is clustered.(not normally distributed)
For model 2, the plot is comparitively scattered and there is no outlier as such.(approximately normally distributed)



<h2>Question 3</h2>
R-squared is a statistical measure that represents the goodness of fit of a regression model. The ideal value for r-square is 1. 
The closer the value of r-square to 1, the better is the model fitted.
R-square is a comparison of residual sum of squares (SSres) with total sum of squares(SStot). 
Total sum of squares is calculated by summation of squares of perpendicular distance between data points and the average line.

The R-squared values are:
Model 1:0.7241
Model 2:0.747

Both R-squared values are very similar suggesting both models are equally good but the log-transformed model seems to be a little better.

R-squared is not a reliable measure as it can be close to 1 for a completely wrong predictive model too.


